{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wine.data','r') as f:\n",
    "    x = f.read().split()\n",
    "    \n",
    "# convert text to a list\n",
    "txt_list = []\n",
    "for i in range(0,len(x)):\n",
    "    y = x[i].split(',')\n",
    "    for j in range(0,len(y)):\n",
    "        y[j] = float(y[j])\n",
    "    txt_list.extend([y])\n",
    "    \n",
    "wine_data = np.array(txt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split wine data into the target and data\n",
    "labels = wine_data[:,0]\n",
    "wine_data = wine_data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data_train = wine_data[labels <= 2.5] # get rid of index 3\n",
    "label_train     = labels[labels<=2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn labels into binary\n",
    "label_train[label_train ==1] = 0\n",
    "label_train[label_train ==2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130, 13), (130,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data_train.shape,label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data_train_norm = minmax.fit_transform(wine_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklr = LR(C = 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\Anaconda2\\envs\\TensorFlowTho\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklr.fit(wine_data_train_norm,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.992007221626415e-16\n"
     ]
    }
   ],
   "source": [
    "#### LOG LOSS\n",
    "loss = - 1*sk.metrics.log_loss(sklr.predict(wine_data_train_norm),label_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklr.score(wine_data_train_norm,label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MY Section ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(weights,x,b):\n",
    "#     print(weights.T@x,b)\n",
    "    sig = (1/(1+math.exp(-1*(weights.T@x+b))))\n",
    "\n",
    "    return(sig)\n",
    "\n",
    "\n",
    "\n",
    "def likelihood(train_data,target_data,weights,b):\n",
    "    #print(weights.shape,train_data[0].shape)\n",
    "    L = 0\n",
    "    for i in range(len(train_data)):\n",
    "        L += (target_data[i]*math.log(sigmoid(weights,train_data[i],b))+\\\n",
    "             (1-target_data[i])*math.log(sigmoid(weights,-1*train_data[i],-b)))   \n",
    "    return -L\n",
    "\n",
    "# def log_loss(X,Y,W,b):\n",
    "#     l_loss = 0\n",
    "#     #print(W.T.shape,X[0].shape)\n",
    "#     for i in range(len(Y)):\n",
    "#         loss = np.log(1+np.exp((-Y[i]*(W.T@X[i]+b))))\n",
    "#         l_loss +=loss\n",
    "#     return -1*l_loss\n",
    "\n",
    "\n",
    "\n",
    "# routine to use the errors to calculate the weight update\n",
    "def calc_grad_w(weights,b,train_data,target_data):\n",
    "\n",
    "    \n",
    "    dL_dWi = np.zeros((13,1))\n",
    "    for i in range(len(weights)):\n",
    "        \n",
    "        grad_update = 0 # stores our total weight updates\n",
    "        \n",
    "        for t in range(len(target_data)): # iterate over all of the data_ppoints\n",
    "        \n",
    "            # dL_dWi = (y_t - sigma(wx_t))*x_it\n",
    "            delta = (target_data[t] - sigmoid(weights,train_data[t],b))\\\n",
    "                    *train_data[t,i]\n",
    "            \n",
    "            grad_update +=delta # accumulate the weight update over samples\n",
    "            \n",
    "        # after seeing the whole set, update the weight for the ith weight element\n",
    "        dL_dWi[i]=grad_update \n",
    "        \n",
    "    return dL_dWi,delta\n",
    "\n",
    "\n",
    "def calc_grad_b(weights,b,train_data,target_data):\n",
    "    g_loss_b = 0\n",
    "    for j in range(len(target_data)):\n",
    "        delta = target_data[j] - sigmoid(weights,train_data[j],b)\n",
    "        g_loss_b += delta\n",
    "    return g_loss_b\n",
    "\n",
    "\n",
    "# def grad_loss_w(X,Y,W,b):\n",
    "#     g_loss_w = np.zeros((len(W),1))\n",
    "#     for i in range(len(W)):\n",
    "#         for j in range(len(Y)):\n",
    "#             g_l_w = -(Y[j]*X[j,i])/(1+np.exp(Y[j]*(W.T@X[j]+b)))\n",
    "#             g_loss_w[i] += g_l_w\n",
    "    \n",
    "#     return g_loss_w\n",
    "\n",
    "# def grad_loss_b(X,Y,W,b):\n",
    "#     g_loss_b = 0\n",
    "#     for j in range(len(Y)):\n",
    "#         g_l_b = -(Y[j])/(1+np.exp(Y[j]*(W.T@X[j]+b)))\n",
    "#         g_loss_b += g_l_b\n",
    "#     return g_loss_b\n",
    "\n",
    "def update(var,gradient,rate):\n",
    "    \n",
    "    var_new = var + rate*gradient\n",
    "    return var_new\n",
    "\n",
    "\n",
    "# go in the direction of maximum gradient\n",
    "def alt_no_rep_update(var,gradient,rate,last_index):\n",
    "    \n",
    "    if last_index == None:\n",
    "        max_index = np.argsort(np.abs(gradient),axis = 0)[-1]\n",
    "        #print(max_index)\n",
    "        var[max_index]   = update(var[max_index],gradient[max_index],rate)\n",
    "        new_last_index = max_index\n",
    "    \n",
    "    else: # pick something that wasn't our last index\n",
    "        \n",
    "        max_index = np.argsort(np.abs(gradient),axis = 0)[-1]\n",
    "        if max_index == last_index:\n",
    "            max_index = np.argsort(np.abs(gradient),axis = 0)[-2]\n",
    "        \n",
    "        \n",
    "        var[max_index]   = update(var[max_index],gradient[max_index],rate)\n",
    "        new_last_index = max_index\n",
    "        \n",
    "    return var,new_last_index\n",
    "        \n",
    "\n",
    "def alt_update(var,gradient,rate):\n",
    "\n",
    "    max_index = np.argsort(np.abs(gradient),axis = 0)[-1]   \n",
    "    var[max_index]   = update(var[max_index],gradient[max_index],rate)\n",
    "\n",
    "    return var\n",
    "    \n",
    "\n",
    "def random_update(var,gradient,rate):\n",
    "    \n",
    "    index = np.random.randint(len(var))  \n",
    "    var[index]   = update(var[index],gradient[index],rate)\n",
    "\n",
    "    return var\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, Loss: 90.10913\n",
      "iter 1000, Loss: 0.13588\n",
      "iter 2000, Loss: 0.06236\n",
      "iter 3000, Loss: 0.04052\n",
      "iter 4000, Loss: 0.03006\n",
      "iter 5000, Loss: 0.02395\n",
      "iter 6000, Loss: 0.01993\n",
      "iter 7000, Loss: 0.01710\n",
      "iter 8000, Loss: 0.01500\n",
      "iter 9000, Loss: 0.01338\n",
      "iter 10000, Loss: 0.01209\n",
      "iter 11000, Loss: 0.01105\n",
      "iter 12000, Loss: 0.01018\n",
      "iter 13000, Loss: 0.00946\n",
      "iter 14000, Loss: 0.00883\n",
      "iter 15000, Loss: 0.00830\n",
      "iter 16000, Loss: 0.00783\n",
      "iter 17000, Loss: 0.00742\n",
      "iter 18000, Loss: 0.00706\n",
      "iter 19000, Loss: 0.00674\n",
      "iter 20000, Loss: 0.00645\n",
      "iter 21000, Loss: 0.00619\n",
      "iter 22000, Loss: 0.00596\n",
      "iter 23000, Loss: 0.00574\n",
      "iter 24000, Loss: 0.00555\n",
      "iter 24999, Loss: 0.00540\n"
     ]
    }
   ],
   "source": [
    "# run the program\n",
    "\n",
    "# initialize variables\n",
    "w = np.zeros((13,1))\n",
    "b = 0\n",
    "lr = 1\n",
    "last_index = None\n",
    "likelihood_log_no_rep = []\n",
    "\n",
    "for k in range(25000):\n",
    "    # calculate loss\n",
    "    l = likelihood(wine_data_train_norm,label_train,w,b)\n",
    "    likelihood_log_no_rep.extend([l])\n",
    "    if k % 1000 == 0:\n",
    "        print('iter %i, Loss: %2.5f' % (k,np.round(l,5)))\n",
    "#         print('Loss:  ',np.round(l,4))\n",
    "    \n",
    "    # calculate gradients\n",
    "    w_grad,delta = calc_grad_w(w,b,wine_data_train_norm,label_train)\n",
    "    #print(type(w_grad),w_grad.shape)\n",
    "    b_grad = calc_grad_b(w,b,wine_data_train_norm,label_train)\n",
    "    \n",
    "    # weight update\n",
    "    \n",
    "    w,last_index = alt_no_rep_update(w,w_grad,lr,last_index)\n",
    "    b = update(b,b_grad,lr)\n",
    "\n",
    "print('iter %i, Loss: %2.5f' % (k,np.round(l,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-75.27246962],\n",
       "       [-12.81918578],\n",
       "       [-36.96634808],\n",
       "       [ 52.32470433],\n",
       "       [-10.59492059],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [  4.16669433],\n",
       "       [ -0.36849474],\n",
       "       [  0.        ],\n",
       "       [  0.75021154],\n",
       "       [-30.25651561],\n",
       "       [-47.15341448]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, Loss: 90.10913\n",
      "iter 1000, Loss: 0.13566\n",
      "iter 2000, Loss: 0.06242\n",
      "iter 3000, Loss: 0.04049\n",
      "iter 4000, Loss: 0.02997\n",
      "iter 5000, Loss: 0.02380\n",
      "iter 6000, Loss: 0.01974\n",
      "iter 7000, Loss: 0.01687\n",
      "iter 8000, Loss: 0.01474\n",
      "iter 9000, Loss: 0.01309\n",
      "iter 10000, Loss: 0.01177\n",
      "iter 11000, Loss: 0.01070\n",
      "iter 12000, Loss: 0.00981\n",
      "iter 13000, Loss: 0.00906\n",
      "iter 14000, Loss: 0.00842\n",
      "iter 15000, Loss: 0.00787\n",
      "iter 16000, Loss: 0.00738\n",
      "iter 17000, Loss: 0.00696\n",
      "iter 18000, Loss: 0.00658\n",
      "iter 19000, Loss: 0.00624\n",
      "iter 20000, Loss: 0.00594\n",
      "iter 21000, Loss: 0.00567\n",
      "iter 22000, Loss: 0.00542\n",
      "iter 23000, Loss: 0.00520\n",
      "iter 24000, Loss: 0.00499\n",
      "iter 24999, Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "# initialize variables\n",
    "w = np.zeros((13,1))\n",
    "b = 0\n",
    "lr = 1\n",
    "last_index = None\n",
    "likelihood_log_max = []\n",
    "\n",
    "for k in range(25000):\n",
    "    # calculate loss\n",
    "    l = likelihood(wine_data_train_norm,label_train,w,b)\n",
    "    likelihood_log_max.extend([l])\n",
    "    if k % 1000 == 0:\n",
    "        print('iter %i, Loss: %2.5f' % (k,np.round(l,5)))\n",
    "#         print('Loss:  ',np.round(l,4))\n",
    "    \n",
    "    # calculate gradients\n",
    "    w_grad,delta = calc_grad_w(w,b,wine_data_train_norm,label_train)\n",
    "    #print(type(w_grad),w_grad.shape)\n",
    "    b_grad = calc_grad_b(w,b,wine_data_train_norm,label_train)\n",
    "    \n",
    "    # weight update\n",
    "    \n",
    "    w= alt_update(w,w_grad,lr)\n",
    "    b = update(b,b_grad,lr)\n",
    "\n",
    "print('iter %i, Loss: %2.2f' % (k,np.round(l,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-83.11416504],\n",
       "       [-12.94528801],\n",
       "       [-35.67872192],\n",
       "       [ 52.32470433],\n",
       "       [ -6.50290457],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [ -3.84240469],\n",
       "       [  0.        ],\n",
       "       [  0.76106167],\n",
       "       [-31.16785726],\n",
       "       [-52.47570376]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, Loss: 90.10913\n",
      "iter 1000, Loss: 7.14402\n",
      "iter 2000, Loss: 0.54909\n",
      "iter 3000, Loss: 0.26427\n",
      "iter 4000, Loss: 0.17968\n",
      "iter 5000, Loss: 0.13873\n"
     ]
    }
   ],
   "source": [
    "# run the program\n",
    "\n",
    "# initialize variables\n",
    "w = np.zeros((13,1))\n",
    "b = 0\n",
    "lr = 1\n",
    "last_index = None\n",
    "likelihood_log_random = []\n",
    "\n",
    "for k in range(25000):\n",
    "    # calculate loss\n",
    "    l = likelihood(wine_data_train_norm,label_train,w,b)\n",
    "    likelihood_log_random.extend([l])\n",
    "    if k % 1000 == 0:\n",
    "        print('iter %i, Loss: %2.5f' % (k,np.round(l,5)))\n",
    "#         print('Loss:  ',np.round(l,4))\n",
    "    \n",
    "    # calculate gradients\n",
    "    w_grad,delta = calc_grad_w(w,b,wine_data_train_norm,label_train)\n",
    "    #print(type(w_grad),w_grad.shape)\n",
    "    b_grad = calc_grad_b(w,b,wine_data_train_norm,label_train)\n",
    "    \n",
    "    # weight update\n",
    "    \n",
    "    w = random_update(w,w_grad,lr)\n",
    "    b = update(b,b_grad,lr)\n",
    "\n",
    "print('iter %i, Loss: %2.2f' % (k,np.round(l,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(input_data,target_data,weights,b):\n",
    "    \n",
    "    y_pred = sigmoid(weights,input_data,b)\n",
    "    \n",
    "    if y_pred > .5:\n",
    "        y_pred = 1\n",
    "    else:\n",
    "        y_pred = 0\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "wine_data_train_norm,label_train\n",
    "accuracy = 0\n",
    "    \n",
    "for j in range(len(label_train)):\n",
    "    y_pred = predict_y(wine_data_train_norm[j],label_train,w,b) # returns 0 or 1\n",
    "\n",
    "    if y_pred == label_train[j]:\n",
    "        accuracy +=1\n",
    "\n",
    "# normalize the accuracy - out of 1400\n",
    "accuracy=accuracy/len(label_train)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(range(1,len(likelihood_log_random)+1),likelihood_log_random)\n",
    "plt.plot(range(1,len(likelihood_log_random)+1),likelihood_log_max)\n",
    "plt.plot(range(1,len(likelihood_log_random)+1),likelihood_log_no_rep)\n",
    "plt.plot(range(1,len(likelihood_log_random)+1),[.000000001]*len(likelihood_log_random))\n",
    "# plt.xscale('log')\n",
    "plt.legend(['random', 'max_grad', 'max_no_rep','true L*'])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
